<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

  <property>
    <name>mapreduce.shuffle.port</name>
    <value>13562</value>
  </property>

  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>

  <property>
    <description>List of directories to store localized files in.</description>
    <name>yarn.nodemanager.local-dirs</name>
    <value>{{yarn_nodemanager_local_dirs}}</value>
  </property>

  <property>
    <description>Where to store container logs.</description>
    <name>yarn.nodemanager.log-dirs</name>
    <value>{{yarn_nodemanager_log_dirs}}</value>
  </property>

  <property>
    <description>Where to aggregate logs to.</description>
    <name>yarn.nodemanager.remote-app-log-dir</name>
{% if groups['namenodes']|count > 1 %}
    <value>hdfs://{{ cluster_name }}/var/log/hadoop-yarn/apps</value>
    {% else %}
    <value>hdfs://{{ groups['namenodes'][0] }}:8020/var/log/hadoop-yarn/apps</value>
{% endif %}
  </property>

  <property>
    <description>Classpath for typical applications.</description>
    <name>yarn.application.classpath</name>
    <value>
        $HADOOP_CONF_DIR,
        $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
        $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
        $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,
        $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
    </value>
  </property>

  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>{{ groups['yarnresourcemanager'][0] }}</value>
  </property>

  <property>
    <name>yarn.resourcemanager.bind-host</name>
    <value>0.0.0.0</value>
  </property>

  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>{{ yarn_nodemanager_resource_memory }}</value>
  </property>

  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>2048</value>
  </property>

  <property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>2</value>
    <description>The minimum allocation for every container request at the RM, in terms of virtual CPU cores. Requests lower than this won't take effect, and the specified value will get allocated the minimum.</description>
  </property>

  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>{{ yarn_nodemanager_resource_cpu }}</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
  </property>

  {% if groups['yarnresourcemanager']|count > 1 %}
  <property>
    <name>yarn.resourcemanager.connect.retry-interval.ms</name>
    <value>2000</value>
  </property>

  <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>yarn.resourcemanager.ha.automatic-failover.embedded</name>
    <value>true</value>
  </property>

  <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>yarn-{{ cluster_name }}</value>
  </property>

  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>{% for item in groups['yarnresourcemanager'] -%}
              {{ item }}{% if not loop.last %},{% endif %}
                        {%- endfor %}</value>
  </property>

  <property>
    <name>yarn.resourcemanager.ha.id</name>
    <value>{{ ansible_hostname }}</value>
  </property>

  <property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  </property>

  <property>
    <name>yarn.resourcemanager.zk-address</name>
    <value>{% for item in groups['zookeepernodes'] -%}
          {{ item }}:2181{% if not loop.last %},{% endif %}
          {%- endfor %}</value>
  </property>

  <property>
    <name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>
    <value>5000</value>
  </property>

  <property>
    <name>yarn.resourcemanager.work-preserving-recovery.enabled</name>
    <value>true</value>
  </property>

{% for item in groups['yarnresourcemanager'] %}
  <property>
    <name>yarn.resourcemanager.address.{{ item }}</name>
    <value>{{ item }}:8032</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.address.{{ item }}</name>
    <value>{{ item }}:8030</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.https.address.{{ item }}</name>
    <value>{{ item }}:8090</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.address.{{ item }}</name>
    <value>{{ item }}:8088</value>
  </property>

  <property>
    <name>yarn.resourcemanager.resource-tracker.address.{{ item }}</name>
    <value>{{ item }}:8031</value>
  </property>

  <property>
    <name>yarn.resourcemanager.admin.address.{{ item }}</name>
    <value>{{ item }}:8033</value>
  </property>
{% endfor %}
{% endif %}

</configuration>
